Subject: RFC: Explicit memory management, iteration 0

- there is no alloc/free from within the workflow, this is a meta-issue and solved on the meta level. The virtual memory provides alloc/free that work with memory handles like now. A typical run would allocate memory, run the workflow with the handle (or mabye more than one) as additional parameter and later free the memory. Also we can re-use the same handle in different workflows or with the same workflow several times without freeing it in between.

<struct name="global_memory_range">
  <field name="handle" type="global_memory_handle_type"/>
  <field name="offset" type="unsigned long"/>
  <field name="size" type="unsigned long"/>
</struct>

<struct name="local_memory_range">
  <field name="buffer_name" type="string"/>
  <field name="offset" type="unsigned long"/>
  <field name="size" type="unsigned long"/>
</struct>

- new tag between <in/out/inout> and <module>

    <memory-buffer name="data">
      <size>/* expression of type unsigned long */</size>
    </memory-buffer>

  Expression will be evaluated before the module gets executed with the input token context of the module bind to the context. Changes to input variables are invisible in the module call.

- new tags as child of <module>

    <memory-get>
      <global>/* list of global_memory_range */</global>
      <local>/* list of local_memory_range */</local>
    </memory-get>

    <memory-put>
      <local/>
      <global/>
    </memory-put>

  memory-get: Expression will be evaluated before the module gets executed with the input token context of the module bind to the context. Changes to input variables are invisible in the module call.

  memory-put: Expression will be evaluated with the output context of the module call. Changes to the output variables are not visible in the workflow.

    <memory-getput> == <memory-get><memory-put>

- example: inplace transformation

    <in name="config" type="config"/>
    <in name="slot" type="slot"/>

    <memory-buffer name="data">
      <size>
        ${config.slot_size}
      </size>
    </memory-buffer>

    <memory-getput>
      <global>
        List ( Struct [ handle := ${config.handle}
                      , offset := ${slot.id} * ${config.slot_size}
                      , size := ${config.slot_size}
                      ]
             )
      </global>
      <local>
        List ( Struct [ buffer := "data"
                      , offset := 0UL
                      , size := ${config.slot_size}
                      ]
             )
      </local>
    </memory-getput>

    <module name="m" function="transform (data, config)"/>

- example: transformation where output differs in size, can be larger than the input but not larger than config.slot_size

    <in name="config" type="config"/>
    <in name="slot_in" type="slot_filled_partially"/>
    <inout name="slot_out" type="slot_filled_partially"/>

    <memory-buffer name="input">
      <size>
        ${slot_in.used}
      </size>
    </memory-buffer>

    <memory-buffer name="output">
      <size>
        ${config.slot_size}
      </size>
    </memory-buffer>

    <memory-get>
      <global>
        List ( Struct [ handle := ${config.handle}
                      , offset := ${slot_in.id} * ${config.slot_size}
                      , size := ${slot_in.used}
                      ]
             )
      </global>
      <local>
        List ( Struct [ buffer := "input"
                      , offset := 0UL
                      , size := ${slot_in.used}
                      ]
             )
      </local>
    </memory-get>

    <memory-put>
      <local>
        List ( Struct [ buffer := "output"
                      , offset := 0UL
                      , size := ${slot_out.used}
                      ]
             )
      </local>
      <global>
        List ( Struct [ handle := ${config.handle}
                      , offset := ${slot_out.id} * ${config.slot_size}
                      , size := ${slot_out.used}
                      ]
             )
      </global>
    </memory-put>

    <module name="m" function="f (input, output, config, slot_in, slot_out)">
      <code>
        // at the end:
        assert (slot_out.used <= config.slot_size);
      </code>
    </module>

- the generated code would

    eval buffer specifications with copied context_input
    allocate local memory, automatically de-allocated at scope exit
    eval memory-get with copied context_input
    get_memory into buffers, e.g. prefix combine the global list locations into the local list locations
    wait
  unwrap_parameter_to_c
  call function (parameter, buffers)
  wrap_parameter_to_workflow
    eval memory-put with copied context_output
    put_memory from buffers, again prefix style
    wait

  Later (iteration optimize fetch/get) we can make separate function in order to run get/put while other function calls are in progress.

In iteration 0 we just use the existing shared memory to store the data,
throwing if there is not enough space. (This is allocate just checks bounds and de-allocate is a no op).

- later (iteration optimize workflow memory_management) we can identify
and eliminate those puts that are followed by a get.

- later (iteration make the scheduler locality aware) we can transport
the list of memory locations that is accessed by the module call to the
scheduler in order to run the task at the location where the copy costs
are low.

- the applications we know can work with it:

* smartb:
  - the subscriber will not fit with our model anyhow, so it should be a separate program that gets a handle and writes to it, independend of any workflow
  - the polling right now does polling and changes parts of the table, this is not possible in the new model, see link:smartb:polling, conceptual.jpg[] for a sketch on how to implement this within the new model: the poller reads the whole table and returns a list of flags that should be processed. That list is split into separate memory locations that are updated to be in progress and the processing of the message starts. once all elements of the list are updated, the poller can start again. after a certain message has been processed the location with the flag is updated to be free again. There are optimizations possible here (as there are now two more module calls per message), the point is that the model fits.

* frtm: does not use virtual memory right now, we could think of using it, e.g. to store the velocity cube. In this case we need to calculate the portion of the table that is accessed by a module call in before. This will be a geometric part of a cube, e.g. not a single memory location but a list of memory locations. The model must allow to connect a <memory-transfer> to a port of type list<memory_location> too. Note that this simplifies the polling in smartb too.

* parsu: it works with slots everywhere: translation is straight forward

* ufbmig: two kind of data is transported: the salt mask (with a fixed allocation, so no problem here) and intervals of a priori unknown size, the management of the interval, including splitting and joining is done in the workflow already, so no problem here either

* superbin: works with slots: translation is straight forward

* azimuthal: like in the frtm lists of locations to describe n-dimensional structures: translation is straight forward

* asian: does not use virtual memory

* kdm: distributes the config of known size to all nodes -> this is not possible because the term "node" is not known in the workflow. this is not strictly necessary as the config should be known in the workflow and not transported via virtual memory, also it would be enught to store it once, distributing it to all nodes is an optimization already

besides that again memory is accessed in a strided fashion, e.g. to select parts of the travel time table and at some places single memory locations suffice

* mapreduce: transfers blocks of memory for which the size is determined within the module call, e.g.
 - used = fread (...), move (ptr, used);
 - move (ptr, last_pos) where last_pos is calculated before
 - move (ptr, part_slice_info.part_used)
 - move (ptr, partition_ingo.part_used)
 - ...
seems that many transfers have a size that is calculated in the module call. There should be a maximum size in all cases, to the translation could use the maximum size instead. This would be less efficient but still fit with the model. The other approach is to avoid the calculations within the module call like it is done in the ufbmig.

* kth blb: will use the virtual memory to cache blocks of input data and for (partial) results of known size: translation is straight forward
