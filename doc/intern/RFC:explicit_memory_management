Subject: RFC: Explicit memory management, iteration 0

- there is no alloc/free from within the workflow, this is a meta-issue and solved on the meta level. The virtual memory provides alloc/free that work with memory handles like now. A typical run would allocate memory, run the workflow with the handle (or mabye more than one) as additional parameter and later free the memory. Also we can re-use the same handle in different workflows or with the same workflow several times without freeing it in between.

- in the workflow a handle typically is split into memory locations of type

<struct name="memory_location">
  <field name="handle" type="memory_handle"/>
  <field name="offset" type="unsigned long"/>
  <field name="size" type="unsigned long"/>
</struct>

- the addition to the workflow language is a new tag <memory-transfer> that describes memory transfers in terms of locations. Via some in-port, the memory locations come into the module call. The values are copied at the momemt the transition is fired. That means that possible modifications are not influencing to locations that the data is copied to (although the modification of an output port would possible become visible in the workflow). Later the keyword "const" will forbid modifications.

<place name="data_location" type="memory_location"/>
<place name="available_location" type="memory_location"/>
<place name="randomized_data_location" type="memory_location"/>

<transition name="randomize">
  <defun>

    <in name="data_location" type="memory_location"/>
    <in name="available_location_in" type="memory_location"/>
    <out name="available_location_out" type="memory_location"/>
    <out name="randomized_data_location" type="memory_location"/>

    <memory-transfer name="data" from="data_location"/>
    <memory-transfer name="randomized_data" to="available_location_in"/>

    <module name="m"
            function="randomize ( data, data_location
                                , randomized_data, randomized_data_location
                                , available_location_in, available_location_out
                                )
                     "
    >
      <code><![CDATA[
      // prolog, generated
      // data = shmem_malloc (data_location.size);
      // randomized_data = shmem_malloc (available_location_in.size);
      // memcpy (data_location -> data)

      randomized_data_location = available_location_in;

      char* d (data);
      char* r (randomized_data);

      for (int i (0); i < data_location.size; ++i)
      {
        r[i] = d[i] ^ rand();
      }

      available_location_out = data_location;

      // epilog, generated
      // memcpy (randomized_data -> available_location_in)
      // shmem_free (randomized_data);
      // shmem_free (data);
      ]]></code>
    </module>
  </defun>
  <connect-in port="data_location" place="data_location"/>
  <connect-in port="available_location_in" place="available_location"/>
  <connect-out port="available_location_out" place="available_location"/>
  <connect-out port="randomized_data_location"
               place="randomized_data_location"/>
</transition>

- the memory transfer could also be connected to a port of type list<memory_location> to support striped data like in the frtm (see below), e.g.

  <in name="portion_of_velocity_cube" type="list of memory_location"/>
  <memory-transfer name="velocity" from="portion_of_velocity_cube"/>

- the generated code would

  unwrap_parameter_to_c
    get_memory into memory_locations, e.g.
      foreach (transfer t : transfers)
        foreach (location l : transfer)
          get (l)
    wait
  call function (parameter, memory_locations)
    put_memory from memory_locations
    wait
  wrap_parameter_to_workflow

  where get_memory and put_memory are new. Later (iteration optimize
fetch/get) we can make separate function in order to run get/put while
other function calls are in progress.

In iteration 0 we just use the existing shared memory to store the data,
throwing if there is not enough space.

- later (iteration optimize workflow memory_management) we can identify
and eliminate those puts that are followed by a get. This is especially
easy when the memory port is connected with an place directly but can be
done for expressions too.

- later (iteration make the scheduler locality aware) we can transport
the list of memory locations that is accessed by the module call to the
scheduler in order to run the task at the location where the copy costs
are low.

- the applications we know can work with it:

* smartb:
  - the subscriber will not fit with our model anyhow, so it should be a separate program that gets a handle and writes to it, independend of any workflow
  - the polling right now does polling and changes parts of the table, this is not possible in the new model, see link:smartb:polling, conceptual.jpg[] for a sketch on how to implement this within the new model: the poller reads the whole table and returns a list of flags that should be processed. That list is split into separate memory locations that are updated to be in progress and the processing of the message starts. once all elements of the list are updated, the poller can start again. after a certain message has been processed the location with the flag is updated to be free again. There are optimizations possible here (as there are now two more module calls per message), the point is that the model fits.

* frtm: does not use virtual memory right now, we could think of using it, e.g. to store the velocity cube. In this case we need to calculate the portion of the table that is accessed by a module call in before. This will be a geometric part of a cube, e.g. not a single memory location but a list of memory locations. The model must allow to connect a <memory-transfer> to a port of type list<memory_location> too. Note that this simplifies the polling in smartb too.

* parsu: it works with slots everywhere: translation is straight forward

* ufbmig: two kind of data is transported: the salt mask (with a fixed allocation, so no problem here) and intervals of a priori unknown size, the management of the interval, including splitting and joining is done in the workflow already, so no problem here either

* superbin: works with slots: translation is straight forward

* azimuthal: like in the frtm lists of locations to describe n-dimensional structures: translation is straight forward

* asian: does not use virtual memory

* kdm: distributes the config of known size to all nodes -> this is not possible because the term "node" is not known in the workflow. this is not strictly necessary as the config should be known in the workflow and not transported via virtual memory, also it would be enught to store it once, distributing it to all nodes is an optimization already

besides that again memory is accessed in a strided fashion, e.g. to select parts of the travel time table and at some places single memory locations suffice

* mapreduce: transfers blocks of memory for which the size is determined within the module call, e.g.
 - used = fread (...), move (ptr, used);
 - move (ptr, last_pos) where last_pos is calculated before
 - move (ptr, part_slice_info.part_used)
 - move (ptr, partition_ingo.part_used)
 - ...
seems that many transfers have a size that is calculated in the module call. There should be a maximum size in all cases, to the translation could use the maximum size instead. This would be less efficient but still fit with the model. The other approach is to avoid the calculations within the module call like it is done in the ufbmig.

* kth blb: will use the virtual memory to cache blocks of input data and for (partial) results of known size: translation is straight forward
