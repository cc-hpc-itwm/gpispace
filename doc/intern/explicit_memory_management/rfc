Subject: RFC: Explicit memory management, iteration 0

- there is no alloc/free from within the workflow, this is a meta-issue and solved on the meta level. The virtual memory provides alloc/free that work with memory handles like now. A typical run would allocate memory, run the workflow with the handle (or mabye more than one) as additional parameter and later free the memory. Also we can re-use the same handle in different workflows or with the same workflow several times without freeing it in between.

<struct name="global_memory_range">
  <field name="handle" type="global_memory_handle_type"/>
  <field name="offset" type="unsigned long"/>
  <field name="size" type="unsigned long"/>
</struct>

<struct name="local_memory_range">
  <field name="buffer" type="string"/>
  <field name="offset" type="unsigned long"/>
  <field name="size" type="unsigned long"/>
</struct>

- new tag between <in/out/inout> and <module>

    <memory-buffer name="data" [read-only="true"]>
      <size>/* expression of type unsigned long */</size>
    </memory-buffer>

  Expression will be evaluated before the module gets executed with the input token context of the module bind to the context. Changes to input variables are invisible in the module call.

When read-only is set to true, the memory will be made available read only in the module call. This will allow to share read-only data between module calls without fetching them again and again. See use case U2 below.

- new tags as child of <module>

    <memory-get>
      <global>/* list of global_memory_range */</global>
      <local>/* list of local_memory_range */</local>
    </memory-get>

    <memory-put>
      <local/>
      <global/>
    </memory-put>

  memory-get: Expression will be evaluated before the module gets executed with the input token context of the module bind to the context. Changes to input variables are invisible in the module call.

  memory-put: Expression will be evaluated with the output context of the module call. Changes to the output variables are not visible in the workflow.

    <memory-getput> == <memory-get><memory-put>

- example: inplace transformation

    <in name="config" type="config"/>
    <in name="slot" type="slot"/>

    <memory-buffer name="data">
      <size>
        ${config.slot_size}
      </size>
    </memory-buffer>

    <memory-getput>
      <global>
        List ( Struct [ handle := ${config.handle}
                      , offset := ${slot.id} * ${config.slot_size}
                      , size := ${config.slot_size}
                      ]
             )
      </global>
      <local>
        List ( Struct [ buffer := "data"
                      , offset := 0UL
                      , size := ${config.slot_size}
                      ]
             )
      </local>
    </memory-getput>

    <module name="m" function="transform (data, config)"/>

- example: gather two partial filled buffers into a single one

    <in name="config" type="config"/>
    <in name="slot_in_A" type="slot"/>
    <in name="slot_in_B" type="slot"/>
    <in name="slot_out" type="slot"/>

    <condition>
      (${slot_in_A.used} + ${slot_in_B.used}) :le: ${config.slot_size}
    </condition>

    <memory-buffer name="data">
      <size>
        ${slot_in_A.used} + ${slot_in_B.used}
      </size>
    </memory-buffer>

    <memory-get>
      <global>
        List ( Struct [ handle := ${config.handle}
                      , offset := ${slot_in_A.id} * ${config.slot_size}
                      , size := ${slot_in_A.used}
                      ]
             , Struct [ handle := ${config.handle}
                      , offset := ${slot_in_B.id} * ${config.slot_size}
                      , size := ${slot_in_B.used}
                      ]
             )
      </global>
      <local>
        List ( Struct [ buffer := "data"
                      , offset := 0UL
                      , size := ${slot_in_A.used} + ${slot_in_B.used}
                      ]
             )
      </local>
    </memory-get>

    <memory-put>
      <global>
        List ( Struct [ handle := ${config.handle}
                      , offset := ${slot_out.id} * ${config.slot_size}
                      , size := ${slot_in_A.used} + ${slot_in_B.used}
                      ]
             )
      </global>
      <local>
        List ( Struct [ buffer := "data"
                      , offset := 0UL
                      , size := ${slot_in_A.used} + ${slot_in_B.used}
                      ]
             )
      </local>
    </memory-get>

    <module name="m" function="gather (data, config)"/>

    The two global memory buffers are collected coalesced into the local buffer and the one is transferred back to the global memory.

    Note that the <local> part could be described as

      <local>
        List ( Struct [ buffer := "data"
                      , offset := 0UL
                      , size := ${slot_in_A.used}
                      ]
             , Struct [ buffer := "data"
                      , offset := ${slot_in_A.used}
                      , size := ${slot_in_B.used}
                      ]
             )
      </local>

    in the get as well as in the put. Also the <global> part of the put could be described by

      <global>
        List ( Struct [ handle := ${config.handle}
                      , offset := ${slot_out.id} * ${config.slot_size}
                      , size := ${slot_in_A.used}
                      ]
             , Struct [ handle := ${config.handle}
                      , offset := ${slot_out.id} * ${config.slot_size}
                                + ${slot_in_A.used}
                      , size := ${slot_in_B.used}
                      ]
             )
      </global>

    All combinations of these versions would give the very same result (maybe with different numbers of transfer).

- example: transformation where output differs in size, can be larger than the input but not larger than config.slot_size

    <in name="config" type="config"/>
    <in name="slot_in" type="slot_filled_partially"/>
    <inout name="slot_out" type="slot_filled_partially"/>

    <memory-buffer name="input">
      <size>
        ${slot_in.used}
      </size>
    </memory-buffer>

    <memory-buffer name="output">
      <size>
        ${config.slot_size}
      </size>
    </memory-buffer>

    <memory-get>
      <global>
        List ( Struct [ handle := ${config.handle}
                      , offset := ${slot_in.id} * ${config.slot_size}
                      , size := ${slot_in.used}
                      ]
             )
      </global>
      <local>
        List ( Struct [ buffer := "input"
                      , offset := 0UL
                      , size := ${slot_in.used}
                      ]
             )
      </local>
    </memory-get>

    <memory-put>
      <local>
        List ( Struct [ buffer := "output"
                      , offset := 0UL
                      , size := ${slot_out.used}
                      ]
             )
      </local>
      <global>
        List ( Struct [ handle := ${config.handle}
                      , offset := ${slot_out.id} * ${config.slot_size}
                      , size := ${slot_out.used}
                      ]
             )
      </global>
    </memory-put>

    <module name="m" function="f (input, output, config, slot_in, slot_out)">
      <code>
        // at the end:
        assert (slot_out.used <= config.slot_size);
      </code>
    </module>

- the generated code would

    eval buffer specifications with copied context_input
    allocate local memory, automatically de-allocated at scope exit
    eval memory-get with copied context_input
    get_memory into buffers, e.g. prefix combine the global list locations into the local list locations
    wait
  unwrap_parameter_to_c
  call function (parameter, buffers)
  wrap_parameter_to_workflow
    eval memory-put with copied context_output
    put_memory from buffers, again prefix style
    wait

  Later (iteration optimize fetch/get) we can make separate function in order to run get/put while other function calls are in progress.

In iteration 0 we just use the existing shared memory to store the data,
throwing if there is not enough space. (This is allocate just checks bounds and de-allocate is a no op).

- later (iteration optimize workflow memory_management) we can identify
and eliminate those puts that are followed by a get.

- later (iteration make the scheduler locality aware) we can transport
the list of memory locations that is accessed by the module call to the
scheduler in order to run the task at the location where the copy costs
are low.

- use cases that we are aware of and how to handle them

U1 (Dirk) Select large parts in a complicated way from a large structure. One example is a huge velocity cube that is written to disk in an order that differs from the order in memory. The way it is done now roughly copies in a loop parts that fit into the local memory, transpose it and write it to disk afterward.

Solution: The coordination must be done in the workflow. The overhead is negligible as the individual parts can be gathered in parallel. Has the advantage to add knowledge to the workflow and allow for more fault tolerance.

U2 (Dirk) Persistent local storage. One example is a costly and huge accelerator structure that is used very often in small (compared to the size of the accelerator) computations. Copying it every time is way to expensive so it should stay in the worker.

Solution: Ignore the fact that the memory is persistent. Instead rely on the ability of the system to avoid unnecessary copies. This is an optimization that most probably is not available in the first implementations. Also to make the optimization possible the memory must be marked read-only. Without the optimization the workflow still works, but does many copies and is (much/prohibitive) slower.

U3 (Dirk) Is a variation of U1: Non-associative reduction of parts that are to large to fit all into the local memory. The solution now gets the data as required from within the module call.

Solution: Again the management is lifted to the workflow. The intermediate results must be written to the virtual memory. An optimization could eliminate the intermediate copies. The proposition P1 (see below) addresses this use case, just set up an iterator within the workflow and increment it within the module call.

U4 (Daniel) Is an easier case of U1: Select substructures from a larger structure, depending on the geometry of the shots to be computed. Is simpler as U1 as the selected part fits into the local memory.

Solution: The main point here is to find from some geometries the memory locations the module call wants to read and their global and local order. This is a repeated task in many projects and should be included in an early example on how to use the new memory tags.

U5 (Dirk) Is U1 for writers, e.g. one has a small input that is used to produce many larger outputs. One example is a list of shots that shares a common (and expensive) pre-calculation and for each element in the list a large cube is produced and put to the global memory. (Yes, the assumption is that there is enough space in the global memory.)

Solution: Again the flow is modeled in the workflow and more overhead is produced. The common pre-calculation can be stored like in U2 as "persistent" blob, however that introduces some difficulties to the user (serialization/deserialization). An output iterator (see P1) would be the solution that gives much freedom to the application and still let the system know much about memory transfers. In general there is a design space that has full knowledge/no freedom at one end and no knowledge/full freedom (the situation now) at the other.

U6 (Dirk) In seislib there is an abstraction for global memory that has two implementations, one for GPI and one for SDPA! This assumes memory access from within the module call and it is unclear how far this reaches. (It was a failure to allow put/get in the first place, now seislib gone the wrong direction: Instead of removing direct memory access it was wrapped and most probably used more often.) Unfortunately that means that many codes will _not_ work out of the box that claim to do so.

U7 (Dirk) Good to remember that Stencil operation are wide spread. One of the early examples should be a Stencil.

- further propositions

P1 (Dirk) Provide iterators to the module call that can be incremented and  dereferenced. In more general terms this would be a way to tell the system something about the access patterns across many memory locations, e.g. a const_forward_iterator would allow for intelligent pre-fetching of data. It breaks the model as an "active" object is needed that triggers memory operations via the drts context. However, the potential benefit is in the saved overhead to go back and forth from the module call to the workflow again and again, just to do the increment there (implicit).

The proposition is to never allow for void* but use iterators right from the beginning. Question: How to specify the type of the iterator? Would add a type field to the memory buffer.

Contra: Complex data would not allow to specify any kind of iterator.

Pro: The extension in the workflow language seems to be small. Just specify that you are going to do the put/get a number of times. However, the implications for the implementation are big.

- the applications we know can work with it:

* smartb:
  - the subscriber will not fit with our model anyhow, so it should be a separate program that gets a handle and writes to it, independend of any workflow
  - the polling right now does polling and changes parts of the table, this is not possible in the new model, see link:smartb:polling, conceptual.jpg[] for a sketch on how to implement this within the new model: the poller reads the whole table and returns a list of flags that should be processed. That list is split into separate memory locations that are updated to be in progress and the processing of the message starts. once all elements of the list are updated, the poller can start again. after a certain message has been processed the location with the flag is updated to be free again. There are optimizations possible here (as there are now two more module calls per message), the point is that the model fits.

* frtm: does not use virtual memory right now, we could think of using it, e.g. to store the velocity cube. In this case we need to calculate the portion of the table that is accessed by a module call in before. This will be a geometric part of a cube, e.g. not a single memory location but a list of memory locations. The model must allow to connect a <memory-transfer> to a port of type list<memory_location> too. Note that this simplifies the polling in smartb too.

* parsu: it works with slots everywhere: translation is straight forward

* ufbmig: two kind of data is transported: the salt mask (with a fixed allocation, so no problem here) and intervals of a priori unknown size, the management of the interval, including splitting and joining is done in the workflow already, so no problem here either

* superbin: works with slots: translation is straight forward

* azimuthal: like in the frtm lists of locations to describe n-dimensional structures: translation is straight forward

* asian: does not use virtual memory

* kdm: distributes the config of known size to all nodes -> this is not possible because the term "node" is not known in the workflow. this is not strictly necessary as the config should be known in the workflow and not transported via virtual memory, also it would be enught to store it once, distributing it to all nodes is an optimization already

besides that again memory is accessed in a strided fashion, e.g. to select parts of the travel time table and at some places single memory locations suffice

* mapreduce: transfers blocks of memory for which the size is determined within the module call, e.g.
 - used = fread (...), move (ptr, used);
 - move (ptr, last_pos) where last_pos is calculated before
 - move (ptr, part_slice_info.part_used)
 - move (ptr, partition_ingo.part_used)
 - ...
seems that many transfers have a size that is calculated in the module call. There should be a maximum size in all cases, to the translation could use the maximum size instead. This would be less efficient but still fit with the model. The other approach is to avoid the calculations within the module call like it is done in the ufbmig.

* kth blb: will use the virtual memory to cache blocks of input data and for (partial) results of known size: translation is straight forward
