Subject: RFC: Performance model, iteration 0

Actual situation

The agent implements an event-driven architecture.
The scheduling is done online, i.e. the decisions are made without complete
information about the entire problem instance.

The jobs are added one by one, whenever a SubmitJobEvent occurs.
The workers are added one by one, whenever a WorkersRegistrationEvent
is received  by the agent. A worker is removed whenever a network error event
or disconnect is triggered.

Most of the existent online scheduling algorithms focus on the
scheduling of a suite of jobs on single machines.
In GPISpace we deal with a co-scheduling problem:
schedule a job on several workers.

In GPISpace the scheduler assumes that the jobs have associated costs,
which are transfer costs and computational costs.

The transfer costs correspond to the time needed to transfer input or output data
(with a known size) between the virtual memory and the local machine.

The computational costs are considered now to be uniform and they should be adapted
s.t. to more accurately estimate the execution time of the jobs/activities/modules.

Q: How to model the computational cost of a module?

   The user should provide a hint about the execution time of a module.
   This can be a function provided by the user, which is able for a given
   input data and a given architecture type or processing element speed (CPU/GPU)
   to give the approximate execution time.

   Additionally, as argument should also be given the number of processors.
   The reason is that, if the module requires co-scheduling, the computational cost
   in this case is the execution time for running the module in parallel on a set of
   workers (processing elements?) of the specified type.

Q: If a job requires co-scheduling, should one allocate resources with the same
   physical characteristics (speed, particularly)? Should we impose such a
   restriction?
   If not, what are the consequences? In the worst case the workers may have quite
   different (matching) characteristics and the users may have not tested their modules
   on all the hardware combinations.

A: Depends on the application. It is up to the application to specify
valid partitions (e.g. same amount of memory per core is required). As
a rule of thumb the scheduler should try to find a homogeneous
partition and only deliver heterogeneous situations if homogeneous is impossible.

Q: what is the relation between workers and processing elements (cores/GPUs),
   (i.e. 1-to-1, 1-to-n)?

A: What we know about for a certain type of worker:
1:1 (parsu)
1:n (one worker per socket, e.g. SPLOTCH and FRTM)

Q: How can a user specify the computational cost of a module?
A: Provide the name of the method as an argument to the module definition function
   and the name of the library in which it is implemented. Or use XML attributes, etc.

Q: How to proceed if the user has no idea or gives no hint about how to estimate
   the computational cost of a module?

A: Use historical or statistical information. Maintain a table storing for each
   module type/name and eventually the input data size the measured execution time
   from the point of view of the agent (i.e. the time between submission and
   result reception). The stored time is an average of all previous execution
   times of a job of this type.

   Not only the average can be stored, but by storing $n$,
   $S=\sum_{1..n} x_i$ and $Q=\sum_{i=n} x_i*x_i$ we know about
   average and stddev as well. (avg = S / n, stddev == sqrt (Q / n -
   avg * avg))

   Probably this will be the default mode of operation, because:
   1. Reality of application development does not have performance
   model, not even performance estimations and
   2. History shows that 1. never changes.

Q: What requirements can be formulated and should be taken into account  for a
   job by the scheduler?

  The current situation is: the scheduler are submitted jobs with the following
  requirements:
      - the required capabilities
      - the number of workers an activity requires
      - a transfer_cost function associated with that activity
      - a computational_cost for that activity and input data (constant now, by default)
      - the total memory buffer size for an activity

Q: What are the characteristics/properties the workers should communicate to the
   agent upon registration and that are to be used by the scheduler?

   Currently the workers are communicating to the agent the following properties:
    - the set of capabilities
    - the size of the allocated shared memory
    - the host name
    - if is a leaf node or not

  One should probably add:
    - the number of processing elements associated to a worker?
      (to clarify the relation worker - PE, how the co-allocation works
      when a worker has more PEs)
    - the available RAM of the host machine?
    - the processing element type (CPU/GPU)
    - the speed/frequency of a processing element

    All this information should be described in a resource description type class.
    Each worker should have associated such a resource type description.

    Groups of workers will share resources:
    - Memory: All workers on a shared memory domain share that
    memory.
    Example: A calc-worker and a reduce-worker share a NUMA socket
    that provides 32 GiB of memory. A calc-activity requires 20 GiB
    and gets scheduled to the calc-worker. A reduce-activity the
    requires 16 GiB can not be scheduled. Later the calc-activity
    finishs and the reduce-activity is scheduled to the
    reduce-worker. A second calc-activity requires only 12 GiB and
    gets scheduled to the calc-worker while the reduce-activity is
    still running.

Q: What scheduling strategies to use?
   Currently, we have only one strategy: the job is assigned to a set of workers
   minimizing the total cost (execution time), implemented in
   find_job_assignment_minimizing_total_cost.

   Other strategies, would be
    - schedule a job to a set of workers minimizing the standard deviation of the
      total costs associated to the workers.
    - schedule a job to a set of workers s.t. the total cost for each worker is
      under a predefined threshold (wall time?)
    - ignore the costs (consider them uniform) and do work stealing
      (this would correspond to a worker request model).
    - minimize energy costs
      This requires knowledge about the machine status, some machines maybe down
      and makes sense to start them up only if there are a lot of jobs to run
      and the costs are not important.

    The strategy can be specified as a parameter when submitting an activity
