Subject: RFC: Performance model, iteration 0

Actual situation

The agent implements an event-driven architecture.
The scheduling is done online, i.e. the decisions are made without complete
information about the entire problem instance.

The jobs are added one by one, whenever a SubmitJobEvent occurs.
The workers are added one by one, whenever a WorkersRegistrationEvent
is received  by the agent. A worker is removed whenever a network error event
or disconnect is triggered.

Most of the existent online scheduling algorithms focus on the
scheduling of a suite of jobs on single machines.
In GPISpace we deal with a co-scheduling problem:
schedule a job on several workers.

In GPISpace the scheduler assumes that the jobs have associated costs,
which are transfer costs and computational costs.

The transfer costs correspond to the time needed to transfer input or output data
(with a known size) between the virtual memory and the local machine.

The computational costs are considered now to be uniform and they should be adapted
s.t. to more accurately estimate the execution time of the jobs/activities/modules.

Q: How to model the computational cost of a module?

   The user should provide a hint about the execution time of a module.
   This can be a function provided by the user, which is able for a given
   input data and a given architecture type or processing element speed (CPU/GPU)
   to give the approximate execution time.

   Additionally, as argument should also be given the number of processors.
   The reason is that, if the module requires co-scheduling, the computational cost
   in this case is the execution time for running the module in parallel on a set of
   workers (processing elements?) of the specified type.

Q: If a job requires co-scheduling, should one allocate resources with the same
   physical characteristics (speed, particularly)? Should we impose such a
   restriction?
   If not, what are the consequences? In the worst case the workers may have quite
   different (matching) characteristics and the users may have not tested their modules
   on all the hardware combinations.

A: Depends on the application. It is up to the application to specify
valid partitions (e.g. same amount of memory per core is required). As
a rule of thumb the scheduler should try to find a homogeneous
partition and only deliver heterogeneous situations if homogeneous is impossible.

Q: what is the relation between workers and processing elements (cores/GPUs),
   (i.e. 1-to-1, 1-to-n)?

A: What we know about for a certain type of worker:
1:1 (parsu)
1:n (one worker per socket, e.g. SPLOTCH and FRTM)

Q: How can a user specify the computational cost of a module?
A: Provide the name of the method as an argument to the module definition function
   and the name of the library in which it is implemented. Or use XML attributes, etc.

Q: How to proceed if the user has no idea or gives no hint about how to estimate
   the computational cost of a module?

A: Use historical or statistical information. Maintain a table storing for each
   module type/name and eventually the input data size the measured execution time
   from the point of view of the agent (i.e. the time between submission and
   result reception). The stored time is an average of all previous execution
   times of a job of this type.

   Not only the average can be stored, but by storing $n$,
   $S=\sum_{1..n} x_i$ and $Q=\sum_{i=n} x_i*x_i$ we know about
   average and stddev as well. (avg = S / n, stddev == sqrt (Q / n -
   avg * avg))

   Probably this will be the default mode of operation, because:
   1. Reality of application development does not have performance
   model, not even performance estimations and
   2. History shows that 1. never changes.

Q: What requirements can be formulated and should be taken into account  for a
   job by the scheduler?

  The current situation is: the scheduler are submitted jobs with the following
  requirements:
      - the required capabilities
      - the number of workers an activity requires
      - a transfer_cost function associated with that activity
      - a computational_cost for that activity and input data (constant now, by default)
      - the total memory buffer size for an activity

Q: What are the characteristics/properties the workers should communicate to the
   agent upon registration and that are to be used by the scheduler?

   Currently the workers are communicating to the agent the following properties:
    - the set of capabilities
    - the size of the allocated shared memory
    - the host name
    - if is a leaf node or not

  One should probably add:
    - the number of processing elements associated to a worker?
      (to clarify the relation worker - PE, how the co-allocation works
      when a worker has more PEs)
    - the available RAM of the host machine?
    - the processing element type (CPU/GPU)
    - the speed/frequency of a processing element

    All this information should be described in a resource description type class.
    Each worker should have associated such a resource type description.

    Groups of workers will share resources:
    - Memory: All workers on a shared memory domain share that
    memory.
    Example: A calc-worker and a reduce-worker share a NUMA socket
    that provides 32 GiB of memory. A calc-activity requires 20 GiB
    and gets scheduled to the calc-worker. A reduce-activity the
    requires 16 GiB can not be scheduled. Later the calc-activity
    finishs and the reduce-activity is scheduled to the
    reduce-worker. A second calc-activity requires only 12 GiB and
    gets scheduled to the calc-worker while the reduce-activity is
    still running.

Q: What scheduling strategies to use?
   Currently, we have only one strategy: the job is assigned to a set of workers
   minimizing the total cost (execution time), implemented in
   find_job_assignment_minimizing_total_cost.

   Other strategies, would be
    - schedule a job to a set of workers minimizing the standard deviation of the
      total costs associated to the workers.
    - schedule a job to a set of workers s.t. the total cost for each worker is
      under a predefined threshold (wall time?)
    - ignore the costs (consider them uniform) and do work stealing
      (this would correspond to a worker request model).
    - minimize energy costs
      This requires knowledge about the machine status, some machines maybe down
      and makes sense to start them up only if there are a lot of jobs to run
      and the costs are not important.

    The strategy can be specified as a parameter when submitting an activity

MR: Personally I very much like a worker request model and I believe
that randomized distributed work stealing will achieve very good
results for a broad range of applications. As an add on such models
not only tend to have simple implementations but, at the same time,
automatically deal with dynamics and incomplete information.

Known Application Scenarios:
Topology, Size, Dynamics, Granularity, Expectations from good (bad) schedules

- Map&Reduce
  Should scale with the number of nodes.
  Typically, #cores workers are spawned on each node.
  There are 3 types of tasks: load, map and reduce.
  The load is very fast/short. 
  The maps are the largest tasks (seconds to minutes). 
  The reduce tasks are intermediate size tasks.
  Completing a load task results in producing a
  map task. The result of a map task gives birth
  to a number of reduce tasks, typically equal to
  the number of partitions used.
  There are different types of reduce (reduce in memory,
  reduce on disk, etc).
  The ideal would be to have all reduce workers running
  at 100% on reduce tasks.
  Typically, all the tasks require virtual memory
  and work on an input/output buffer (load, map)
  or multiple input buffers and one output buffer,
  which is the case of reduce tasks.
  Each task type works with buffer of a predefined size.
  Problem 1: how to automatically choose the size of
  the buffers? (Currently, set a priori)
  Problem 2: how to decide which number of workers of each
  type to spawn such that to reach the maximum performance?
  
  Q: should we the workers to change the capabilities at runtime,
     in the case more workers of a certain type are needed?
     If yes, information from the workflow is required.
   
  Main criteria for scheduling: all reduce workers are busy.

- FRTM
* runs on >1000 nodes
* runs 5-10 worker per node
  calc = 100% busy, from minutes to some hours, O(10000)
  reduce = busy sometimes, several minutes, O(10000)
  management, cleanup = busy sometimes, some seconds, O(10000)
* does not use virtual memory
* workflow knows about the number of workers and tries to always keep
  the number of activities in the range of two times the number of workers
* (fast) addition and removal nodes of, mostly in bunches of 64 or 128
* (fast) interruption (cancel), calc already installs on_cancel_handler
* uses workflow_response for status report
* main criteria for good schedule: all calc workers are busy all the time

- SPLOTCH
* runs and scales up to 128 nodes
* is expected to run on O(1000) nodes
* runs 4-5 worker per node
  calc = 100% busy, some seconds to minutes, O(1000000)
  load = busy sometimes, several seconds to a minute, O(10000)
  store = busy sometimes, several seconds to a minute, O(10000)
* uses virtual memory
* number of workers does not change during a single run
* jobs consists of small dependency pipeline, e.g. after load some calc
  are triggered and a number of calc releases a store
* main criteria for good schedule: calc workers are busy (excluding
  memory transfer costs)
* wanted: memory shared between workers (more general: shared
  resources, e.g. memory, cpu) = overlapping resources

- Adaptive Stacking
* vmem application
* has a big read-only constant in memory after phase one
* should be integrated with FRTM
* each activity transports significant amount of data from/to the vmem
  => locality is very important, besides that the schedule seems to be
  straight forward
  * load O(10000), some seconds
  * store O(1), some seconds
  * correlate O(10000), some seconds
  * reduce O(10000), some seconds
* number of available activities is more or less constant over the
  runtime, e.g. O(number of slots in virtual memory) ~ O(5-10 * number
  of worker)
* running time of a single activity is correlated with the size of a
  vmem slot
* no worker dynamics

- Parsu
* in theory scales with number of nodes (plain split of input data)
* 2-3 load, store, theoretically overlapping with calc
  5-10 calc, constantly
* vmem for work packages
* constant topo
* load -> calc -> store
* vmem packages freed for load after calc, for calc after store
* good schedule: constant calc, vmem locality for the whole data chain
* wanted: shared memory between workers (scheduler known about sum oif
  available memory and required memory per activity)

- Parsusort
* fine to run on few nodes, algorithmic does not scale to a load of
  nodes (final, singlethreaded step at end)
* few workers per node, threaded for IO, not using vmem, so threads
  better than workers
  only one type of workers, at first all busy with same runtime, then
  log-k parallel activities down to only one active with runtime of
  activities increasing
* number of workers constant
* no fancy new stuff (put_token, …)
* good schedule: equal distribution, as long as file system is equal
  on all nodes, no locality exists.
* nyi but beneficial to resource sharing: removal of nodes after each
  merge level

- ...
